{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Cargar el dataset\n",
    "file_path = \"../data/processed/train_data_processed_imputeKnn_scale.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Inspección inicial\n",
    "print(\"Dimensiones del dataset antes de eliminar atípicos:\", data.shape)\n",
    "print(data.head())\n",
    "\n",
    "# Identificar la variable objetivo y las características\n",
    "target_col = \"target\"\n",
    "features = [col for col in data.columns if col != target_col and col != 'therapeutic_area']  # Excluir 'therapeutic_area'\n",
    "\n",
    "# Codificar columnas categóricas\n",
    "label_encoders = {}  # Almacenar los codificadores para revertir la codificación si es necesario\n",
    "for col in features:\n",
    "    if data[col].dtype == 'object':  # Si la columna es de tipo objeto (categórica)\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "        label_encoders[col] = le  # Guardar el codificador para futuras referencias\n",
    "\n",
    "# Función para eliminar valores atípicos usando el rango intercuartílico (IQR)\n",
    "def remove_outliers(df, cols):\n",
    "    for col in cols:\n",
    "        if np.issubdtype(df[col].dtype, np.number):  # Solo para columnas numéricas\n",
    "            Q1 = df[col].quantile(0.25)  # Primer cuartil\n",
    "            Q3 = df[col].quantile(0.75)  # Tercer cuartil\n",
    "            IQR = Q3 - Q1  # Rango intercuartílico\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# Eliminar valores atípicos por área terapéutica\n",
    "therapeutic_areas = data['therapeutic_area'].unique()\n",
    "data_cleaned = pd.DataFrame()  # Para almacenar los datos sin atípicos\n",
    "\n",
    "for area in therapeutic_areas:\n",
    "    area_data = data[data['therapeutic_area'] == area]\n",
    "    area_data = remove_outliers(area_data, features)  # Eliminar atípicos\n",
    "    data_cleaned = pd.concat([data_cleaned, area_data], ignore_index=True)\n",
    "\n",
    "print(\"Dimensiones del dataset después de eliminar atípicos:\", data_cleaned.shape)\n",
    "\n",
    "# Entrenamiento de modelos para cada área terapéutica\n",
    "models_by_area = {}\n",
    "rmse_by_area = {}\n",
    "\n",
    "for area in therapeutic_areas:\n",
    "    print(f\"\\nEntrenando modelo para el área terapéutica: {area}\")\n",
    "    \n",
    "    # Filtrar los datos por área terapéutica\n",
    "    area_data = data_cleaned[data_cleaned['therapeutic_area'] == area]\n",
    "    X_area = area_data[features]\n",
    "    y_area = area_data[target_col]\n",
    "    \n",
    "    # Dividir en entrenamiento y validación\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_area, y_area, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Definir el modelo XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "\n",
    "    # Definir el espacio de búsqueda de hiperparámetros\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'min_child_weight': [1, 5, 10]\n",
    "    }\n",
    "\n",
    "    # Realizar la búsqueda de hiperparámetros con validación cruzada\n",
    "    grid_search = GridSearchCV(estimator=xgb_model,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring='neg_mean_squared_error', \n",
    "                               cv=3, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Mostrar los mejores parámetros\n",
    "    print(\"Mejores hiperparámetros para el área:\", area)\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # Usar el mejor modelo encontrado\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "    # Realizar predicciones en el conjunto de validación\n",
    "    y_pred = best_xgb_model.predict(X_val)\n",
    "    \n",
    "    # Calcular el RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    rmse_by_area[area] = rmse\n",
    "    \n",
    "    # Almacenar el modelo entrenado\n",
    "    models_by_area[area] = best_xgb_model\n",
    "    \n",
    "    print(f\"RMSE para el área {area}: {rmse:.4f}\")\n",
    "\n",
    "# Resumen del RMSE por área terapéutica\n",
    "print(\"\\nResumen de RMSE por área terapéutica:\")\n",
    "for area, rmse in rmse_by_area.items():\n",
    "    print(f\"Área {area}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Guardar los modelos entrenados en disco\n",
    "import joblib\n",
    "\n",
    "for area, model in models_by_area.items():\n",
    "    model_filename = f\"xgb_model_{area}.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Modelo para el área {area} guardado como {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de submission\n",
    "submission_file_path = \"../data/processed/submission_data_processed_imputeKnn_scale.csv\"\n",
    "submission_data = pd.read_csv(submission_file_path)\n",
    "submission_data_raw = pd.read_csv(\"../data/raw/submission_data.csv\")\n",
    "\n",
    "# Asegurar que las características coincidan con las usadas durante el entrenamiento\n",
    "assert set(features).issubset(submission_data.columns), \"Las características no coinciden entre train y submission\"\n",
    "\n",
    "# Codificar las columnas categóricas en submission_data usando los LabelEncoders entrenados\n",
    "for col, le in label_encoders.items():\n",
    "    if col in submission_data.columns:\n",
    "        # Verificar si hay categorías nuevas\n",
    "        submission_data[col] = submission_data[col].apply(\n",
    "            lambda x: le.transform([x])[0] if x in le.classes_ else -1  # Asignar -1 a valores desconocidos\n",
    "        )\n",
    "\n",
    "# Lista para almacenar las predicciones\n",
    "submission_predictions = []\n",
    "\n",
    "# Iterar sobre cada fila del dataset de submission\n",
    "for _, row in submission_data.iterrows():\n",
    "    # Identificar el área terapéutica de la fila\n",
    "    therapeutic_area = row['therapeutic_area']\n",
    "    \n",
    "    # Seleccionar el modelo correspondiente\n",
    "    if therapeutic_area in models_dict:\n",
    "        model = models_dict[therapeutic_area]\n",
    "        \n",
    "        # Seleccionar las características relevantes para predicción\n",
    "        row_features = row[features].values.reshape(1, -1)  # Convertir en un array 2D\n",
    "        \n",
    "        # Realizar la predicción\n",
    "        prediction = model.predict(row_features)[0]  # Extraer el valor predicho\n",
    "    else:\n",
    "        # Asignar NaN si no hay modelo para el área terapéutica\n",
    "        prediction = np.nan\n",
    "        print(f\"Advertencia: No se encontró modelo para el área terapéutica '{therapeutic_area}'. Se asigna NaN.\")\n",
    "\n",
    "    # Almacenar la predicción\n",
    "    submission_predictions.append(prediction)\n",
    "\n",
    "# Agregar las predicciones como una nueva columna en el DataFrame de submission\n",
    "submission_data['prediction'] = submission_predictions\n",
    "submission_data['cluster_nl'] = submission_data_raw['cluster_nl']\n",
    "submission_data['date'] = submission_data_raw['date']\n",
    "\n",
    "submission_data = submission_data[['date', 'cluster_nl', 'prediction']]\n",
    "\n",
    "# Guardar las predicciones en un archivo CSV\n",
    "submission_output_path = \"../data/processed/submission_predictions.csv\"\n",
    "submission_data.to_csv(submission_output_path, index=False)\n",
    "print(f\"Predicciones para submission guardadas en: {submission_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de submission\n",
    "submission_file_path = \"../data/processed/submission_data_processed_imputeKnn_scale.csv\"\n",
    "submission_data = pd.read_csv(submission_file_path)\n",
    "\n",
    "# Asegurar que las características coincidan con las usadas durante el entrenamiento\n",
    "assert set(features).issubset(submission_data.columns), \"Las características no coinciden entre train y submission\"\n",
    "\n",
    "# Codificar las columnas categóricas en submission_data usando los LabelEncoders entrenados\n",
    "for col, le in label_encoders.items():\n",
    "    if col in submission_data.columns:\n",
    "        # Verificar si hay categorías nuevas\n",
    "        submission_data[col] = submission_data[col].apply(\n",
    "            lambda x: le.transform([x])[0] if x in le.classes_ else -1  # Asignar -1 a valores desconocidos\n",
    "        )\n",
    "\n",
    "\n",
    "# Lista para almacenar las predicciones\n",
    "submission_predictions = []\n",
    "\n",
    "# Iterar sobre cada fila del dataset de submission\n",
    "for _, row in submission_data.iterrows():\n",
    "    # Identificar el área terapéutica de la fila\n",
    "    therapeutic_area = row['therapeutic_area']\n",
    "    \n",
    "    # Seleccionar el modelo correspondiente\n",
    "    if therapeutic_area in models_dict:\n",
    "        model = models_dict[therapeutic_area]\n",
    "        \n",
    "        # Seleccionar las características relevantes para predicción\n",
    "        row_features = row[features].values.reshape(1, -1)  # Convertir en un array 2D\n",
    "        \n",
    "        # Realizar la predicción\n",
    "        prediction = model.predict(row_features)[0]  # Extraer el valor predicho\n",
    "    else:\n",
    "        # Asignar NaN si no hay modelo para el área terapéutica\n",
    "        prediction = np.nan\n",
    "        print(f\"Advertencia: No se encontró modelo para el área terapéutica '{therapeutic_area}'. Se asigna NaN.\")\n",
    "\n",
    "    # Almacenar la predicción\n",
    "    submission_predictions.append(prediction)\n",
    "\n",
    "# Agregar las predicciones como una nueva columna en el DataFrame de submission\n",
    "submission_data['prediction'] = submission_predictions\n",
    "\n",
    "# Guardar las predicciones en un archivo CSV\n",
    "submission_output_path = \"../data/processed/submission_predictions.csv\"\n",
    "submission_data.to_csv(submission_output_path, index=False)\n",
    "print(f\"Predicciones para submission guardadas en: {submission_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
